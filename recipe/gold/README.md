# Recipe: GOLD (General Online Logit Distillation) Trainer

**Last updated:** 2025-12-09

## 1. Overview

GOLD (General Online Logit Distillation) is an on-policy knowledge distillation algorithm that extends GKD (Generalized Knowledge Distillation) with:

1. **Generalized Jensen-Shannon Divergence (JSD)**: Instead of pure KL divergence, GOLD uses JSD with a configurable `beta` parameter that interpolates between forward and reverse KL.

2. **On-policy/Off-policy Mixing**: GOLD introduces a `lambda` parameter to control the mix between on-policy samples (generated by the student) and off-policy samples (from the dataset).

3. **Temperature Scaling**: Explicit temperature parameter for softmax distribution smoothing.

This implementation is built on verl's Ray-based distributed training infrastructure and reuses components from the GKD recipe.

## 2. Algorithm

### 2.1 Generalized JSD Loss

The GOLD loss function is the Generalized Jensen-Shannon Divergence:

```
JSD_β(P||Q) = β * KL(P||M) + (1-β) * KL(Q||M)

where:
  M = β * P + (1-β) * Q
  P = teacher distribution
  Q = student distribution
  β = interpolation coefficient (default: 0.5)
```

**Effect of β:**
- `β = 0.5`: Symmetric JSD (balanced between forward and reverse KL)
- `β → 1.0`: Approaches forward KL - student covers all teacher modes (avoids mode collapse)
- `β → 0.0`: Approaches reverse KL - student focuses on high-probability modes

### 2.2 On-policy/Off-policy Mixing

With probability `λ` (lmbda):
- Generate responses on-policy from the student model
- Use these fresh samples for distillation

With probability `1-λ`:
- Use completions from the dataset (off-policy)
- More sample-efficient but may have distribution mismatch

**Effect of λ:**
- `λ = 1.0`: Pure on-policy (like GKD) - best for reducing distribution mismatch
- `λ = 0.0`: Pure off-policy (like SFT) - most sample efficient
- `λ = 0.5`: Mixed - balance between sample efficiency and distribution matching

### 2.3 Temperature Scaling

Temperature `T` controls the sharpness of probability distributions:

```
P_T(x) = softmax(logits / T)
```

- `T < 1.0`: Sharper distributions (more confident)
- `T = 1.0`: Original distributions
- `T > 1.0`: Smoother distributions (more uniform)

## 3. Architecture

GOLD reuses most of the GKD infrastructure:

```
┌─────────────────────────────────────────────────────────────────┐
│                    GOLD Training Pipeline                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  TaskRunner (Driver)                                            │
│  ├─ Hydra config management                                     │
│  ├─ Ray cluster initialization                                  │
│  └─ GOLDTrainer.fit()                                           │
│      │                                                          │
│      ├─ λ-controlled sample selection                           │
│      │   ├─ On-policy: _async_gen_next_batch()                  │
│      │   │   ├─ sync_rollout_weights()                          │
│      │   │   └─ rollout_wg.async_generate_sequences()           │
│      │   └─ Off-policy: _prepare_off_policy_batch()             │
│      │       └─ Use dataset completions                         │
│      │                                                          │
│      ├─ Teacher knowledge retrieval                             │
│      │   └─ teacher_client.submit()                             │
│      │                                                          │
│      └─ Training Loop                                           │
│          ├─ Retrieve (batch, gen_output, teacher_output)        │
│          ├─ actor_wg.update_actor(merged_batch)                 │
│          │   └─ GOLDActor.update_policy()                       │
│          │       ├─ forward_backward_batch()                    │
│          │       │   └─ vocab_parallel_jsd_divergence()         │
│          │       └─ optimizer.step()                            │
│          └─ Log metrics (including GOLD-specific)               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## 4. Key Components

### 4.1 JSD Loss (`megatron_jsd_loss.py`)

- `vocab_parallel_jsd_divergence()`: Main entry point for JSD loss
- Tensor-parallel aware computation
- Supports `beta` and `temperature` parameters
- Numerically stable with max-shifting

### 4.2 GOLD Trainer (`ray_trainer.py`)

- `GOLDTrainer`: Extends base trainer with GOLD logic
- `_should_use_on_policy()`: λ-based sample selection
- Tracks on-policy vs off-policy statistics
- Logs GOLD-specific metrics

### 4.3 GOLD Workers (`megatron_workers.py`)

- `GOLDActor`: Actor with JSD loss computation
- `MegatronGOLDActorWorker`: Ray worker wrapper
- `MegatronGOLDRolloutWorker`: Rollout worker (same as GKD)

### 4.4 Reused from GKD

- Teacher server (`teacher/`)
- Teacher utilities (`teacher_utils.py`)
- Megatron utilities (`megatron_utils.py`)
- Schedulers (one_step_off, two_step_off)

## 5. Configuration

### 5.1 GOLD-Specific Parameters

```yaml
gold:
  # JSD interpolation (0.0-1.0)
  beta: 0.5

  # On-policy fraction (0.0-1.0)
  lmbda: 0.5

  # Softmax temperature
  temperature: 0.9

  # Sequence-level KD (not yet implemented)
  seq_kd: false
```

### 5.2 Common Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `gold.beta` | JSD interpolation coefficient | 0.5 |
| `gold.lmbda` | On-policy sample fraction | 0.5 |
| `gold.temperature` | Softmax temperature | 0.9 |
| `trainer.scheduler` | `one_step_off` or `two_step_off` | one_step_off |
| `trainer.n_gpus_per_node` | GPUs for actor | 4 |
| `rollout.n_gpus_per_node` | GPUs for rollout | 4 |

## 6. Usage

### 6.1 Launch Teacher Server

Before training, start the teacher server (same as GKD):

```bash
cd recipe/gold/teacher
bash start_server.sh
```

Verify:
```bash
telnet localhost 15555
```

### 6.2 Run GOLD Training

**Basic run:**
```bash
python -m recipe.gold.main_gold \
  --config-path=recipe/gold/config \
  --config-name=gold_trainer \
  actor_rollout_ref.model.path=/path/to/MODEL \
  data.train_files=/path/to/train.parquet \
  trainer.total_epochs=10 \
  trainer.n_gpus_per_node=4 \
  rollout.n_gpus_per_node=2 \
  actor_rollout_ref.teacher.server_ip=127.0.0.1 \
  actor_rollout_ref.teacher.server_port=15555
```

**With GOLD parameters:**
```bash
python -m recipe.gold.main_gold \
  --config-path=recipe/gold/config \
  --config-name=gold_trainer \
  actor_rollout_ref.model.path=/path/to/MODEL \
  data.train_files=/path/to/train.parquet \
  gold.beta=0.7 \
  gold.lmbda=0.8 \
  gold.temperature=0.9 \
  trainer.scheduler=two_step_off
```

## 7. Metrics

GOLD logs additional metrics beyond GKD:

| Metric | Description |
|--------|-------------|
| `gold/on_policy_ratio` | Actual on-policy sample ratio |
| `gold/beta` | Current β value |
| `gold/lmbda` | Current λ value |
| `gold/temperature` | Current temperature |
| `gold/is_on_policy` | Whether current batch is on-policy |
| `actor/jsd_loss` | JSD loss value |

Standard metrics are also available:
- `response_seq_len/*`: Response length statistics
- `actor/grad_norm`, `actor/lr`: Training dynamics
- `perf/mfu/actor`: Model FLOP utilization
- `timing/*`: Pipeline timing metrics

## 8. Comparison with GKD

| Feature | GKD | GOLD |
|---------|-----|------|
| Loss | KL(P_teacher \|\| P_student) | β*KL(P\|\|M) + (1-β)*KL(Q\|\|M) |
| On-policy | Always (100%) | λ-controlled |
| Temperature | Implicit | Explicit parameter |
| Interpolation | Fixed | β parameter |

## 9. Tips

1. **Start with GKD settings**: Use `beta=1.0`, `lmbda=1.0` to match GKD behavior, then tune.

2. **For mode coverage**: Increase `beta` toward 1.0 to ensure student covers teacher modes.

3. **For efficiency**: Decrease `lmbda` to use more off-policy samples (faster but may have distribution mismatch).

4. **For softer distillation**: Increase `temperature` to make distributions more uniform.

5. **Monitor `on_policy_ratio`**: Verify it matches your `lmbda` setting.

## 10. References

- [TRL GOLD Trainer](https://huggingface.co/docs/trl/main/gold_trainer)
- [On-Policy Distillation Demo](https://huggingface.co/spaces/HuggingFaceH4/on-policy-distillation)
- [GKD Paper: "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes"](https://arxiv.org/abs/2306.13649)
