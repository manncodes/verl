# Analyze Rollouts - Reward Calculation & Difficulty Filtering

This script takes existing rollouts (generated by `generate_rollouts.py`) and calculates rewards, computes metrics, and performs difficulty bucketing.

## Why Separate Reward Calculation?

**Separation of Concerns**: Generation is slow, analysis is fast!

1. **Generate rollouts once** (slow, GPU-intensive)
   ```bash
   python scripts/generate_rollouts.py --use_vllm --data_path data.parquet --output_path rollouts.parquet
   ```

2. **Analyze many times** (fast, CPU-based)
   ```bash
   python scripts/analyze_rollouts.py --rollouts rollouts.parquet --model_path /path/to/model --output_dir ./filtered/
   ```

This allows you to:
- Try different bucketing strategies without regenerating
- Reuse the same rollouts for multiple experiments
- Save time and compute resources

## Quick Start

```bash
# Analyze rollouts with default settings (percentile bucketing)
python scripts/analyze_rollouts.py \
    --rollouts rollouts.parquet \
    --model_path /path/to/model \
    --output_dir ./difficulty_results

# Use pass_rate bucketing strategy
python scripts/analyze_rollouts.py \
    --rollouts rollouts.parquet \
    --model_path /path/to/model \
    --output_dir ./difficulty_results \
    --bucketing_strategy pass_rate

# Use adaptive (K-means clustering) bucketing
python scripts/analyze_rollouts.py \
    --rollouts rollouts.parquet \
    --model_path /path/to/model \
    --output_dir ./difficulty_results \
    --bucketing_strategy adaptive
```

## Full Workflow Example

```bash
# Step 1: Generate rollouts (do this once)
python scripts/generate_rollouts.py \
    --use_vllm \
    --vllm_url http://localhost:8000/v1 \
    --data_path gsm8k_train.parquet \
    --output_path gsm8k_rollouts.parquet \
    --num_samples 10 \
    --temperature 0.7

# Step 2: Analyze with percentile bucketing
python scripts/analyze_rollouts.py \
    --rollouts gsm8k_rollouts.parquet \
    --model_path Qwen/Qwen2.5-7B-Instruct \
    --output_dir ./results_percentile \
    --bucketing_strategy percentile

# Step 3: Try different bucketing (no need to regenerate!)
python scripts/analyze_rollouts.py \
    --rollouts gsm8k_rollouts.parquet \
    --model_path Qwen/Qwen2.5-7B-Instruct \
    --output_dir ./results_pass_rate \
    --bucketing_strategy pass_rate

# Step 4: Try adaptive bucketing
python scripts/analyze_rollouts.py \
    --rollouts gsm8k_rollouts.parquet \
    --model_path Qwen/Qwen2.5-7B-Instruct \
    --output_dir ./results_adaptive \
    --bucketing_strategy adaptive
```

## Command-Line Arguments

### Required Arguments

- `--rollouts`: Path to rollouts parquet/jsonl file (output from `generate_rollouts.py`)
- `--model_path`: Path to model checkpoint (needed for tokenizer)
- `--output_dir`: Directory to save analysis results

### Optional Arguments

- `--bucketing_strategy`: Strategy for difficulty bucketing (default: `percentile`)
  - `percentile`: Bucket by percentile of mean@5
  - `pass_rate`: Bucket by pass rate (reward >= 0.5)
  - `mean_reward`: Bucket by mean reward across all samples
  - `adaptive`: K-means clustering on multiple features

- `--reward_fn_key`: Key in data containing reward model info (default: `reward_model`)

## Input Format

The script expects rollouts in parquet or jsonl format with the following columns:

- `prompt`: The input prompt
- `response`: The generated response
- `data_idx`: Index of the original problem
- `sample_idx`: Index of the sample (0 to num_samples-1)
- `original_*`: Original data columns (e.g., `original_answer`, `original_question`)

This matches the output format from `generate_rollouts.py`.

## Output Files

The script creates the following files in the output directory:

### 1. `rollouts_with_rewards.parquet`
All rollouts with an additional `reward` column.

### 2. `problem_metrics.json`
Per-problem metrics including:
- `mean@1`, `mean@3`, `mean@5`, `mean@10`, `mean@20`: Mean of top-k rewards
- `pass_rate`: Percentage of samples with reward >= 0.5
- `mean_reward`: Mean reward across all samples
- `max_reward`, `min_reward`, `std_reward`: Statistics
- `num_samples`: Number of samples for this problem

### 3. `problem_buckets.json`
Mapping of `data_idx` to bucket name (e.g., "hard", "medium", "easy").

### 4. `rollouts_{bucket_name}.parquet`
Separate parquet files for each difficulty bucket (e.g., `rollouts_hard.parquet`, `rollouts_easy.parquet`).

### 5. `summary.json`
Overall analysis summary including:
- Total problems and rollouts
- Bucketing strategy used
- Bucket distribution (how many problems in each bucket)
- Overall metrics (mean pass rate, mean reward, mean@5)

## Bucketing Strategies

### 1. Percentile (default)
Buckets problems by percentile of `mean@5`:
- **hard**: Bottom 25% (0-25th percentile)
- **medium-hard**: 25-50th percentile
- **medium-easy**: 50-75th percentile
- **easy**: Top 25% (75-100th percentile)

### 2. Pass Rate
Buckets problems by pass rate (percentage of samples with reward >= 0.5):
- **very_hard**: 0% pass rate
- **hard**: 0-25% pass rate
- **medium**: 25-50% pass rate
- **easy**: 50-75% pass rate
- **very_easy**: 75-100% pass rate

### 3. Mean Reward
Buckets problems by quartile of mean reward:
- **very_hard**: Bottom 25%
- **hard**: 25-50%
- **medium**: 50-75%
- **easy**: Top 25%

### 4. Adaptive (K-means)
Uses K-means clustering on multiple features:
- `mean@5`: Mean of top-5 rewards
- `pass_rate`: Percentage passing
- `std_reward`: Standard deviation of rewards

Creates 4 clusters and sorts them by difficulty based on mean@5.

## Metrics Explained

### Mean@k
Mean of the top-k rewards for a problem. This measures the "best case" performance:
- `mean@1`: Best single sample (max reward)
- `mean@5`: Mean of best 5 samples
- Higher values = easier problem (model can solve it)

### Pass Rate
Percentage of samples that "pass" (reward >= 0.5):
- 0.0 = No samples passed (very hard)
- 1.0 = All samples passed (very easy)

### Mean Reward
Average reward across all samples:
- Captures overall difficulty
- Affected by both best and worst samples

### Standard Deviation
Variance in rewards:
- High std = Inconsistent (some good, some bad)
- Low std = Consistent performance

## Example Output

```
============================================================
ANALYSIS SUMMARY
============================================================
Total problems: 1000
Total rollouts: 10000
Bucketing strategy: percentile

Bucket distribution:
  hard: 250 problems
  medium-hard: 250 problems
  medium-easy: 250 problems
  easy: 250 problems

Overall metrics:
  Mean pass rate: 0.634
  Mean reward: 0.587
  Mean@5: 0.723
============================================================
```

## Advanced Usage

### Using with Custom Reward Functions

If your original data has a custom reward function key:

```bash
python scripts/analyze_rollouts.py \
    --rollouts rollouts.parquet \
    --model_path /path/to/model \
    --output_dir ./results \
    --reward_fn_key custom_reward_data
```

### Combining with Different Generation Settings

You can analyze the same rollouts generated with different temperatures:

```bash
# Generate with low temperature (deterministic)
python scripts/generate_rollouts.py --temperature 0.1 --output_path rollouts_low_temp.parquet ...

# Generate with high temperature (creative)
python scripts/generate_rollouts.py --temperature 1.0 --output_path rollouts_high_temp.parquet ...

# Analyze both
python scripts/analyze_rollouts.py --rollouts rollouts_low_temp.parquet --output_dir ./low_temp_results ...
python scripts/analyze_rollouts.py --rollouts rollouts_high_temp.parquet --output_dir ./high_temp_results ...
```

## Performance

Reward calculation is CPU-based and relatively fast:
- ~100-500 rollouts/second depending on sequence length
- No GPU required
- Can run on login nodes or CPU-only machines

## Troubleshooting

### Error: "KeyError: 'original_X'"

Make sure your rollouts were generated with `generate_rollouts.py`, which preserves original data columns with `original_` prefix.

### Error: "No module named 'sklearn'"

Install scikit-learn for adaptive bucketing:
```bash
pip install scikit-learn
```

### Low rewards for all problems

Check that:
1. Your reward function is configured correctly
2. The `reward_fn_key` matches your data
3. Original data has the necessary fields for reward calculation (e.g., ground truth answers)

## See Also

- `README_generate_rollouts.md` - Generating rollouts
- `README_difficulty_filter.md` - Full pipeline (generation + analysis in one script)
