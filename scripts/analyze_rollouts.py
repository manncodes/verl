#!/usr/bin/env python3
"""
Analyze existing rollouts - calculate rewards and perform difficulty filtering.

This script takes rollouts generated by generate_rollouts.py and:
1. Calculates rewards using verl's reward infrastructure
2. Computes mean@k metrics per problem
3. Performs difficulty bucketing using various strategies
4. Outputs filtered/bucketed results

Workflow:
    Step 1: Generate rollouts (slow, do once)
        python scripts/generate_rollouts.py --use_vllm --data_path data.parquet --output_path rollouts.parquet

    Step 2: Analyze rollouts (fast, can run many times)
        python scripts/analyze_rollouts.py --rollouts rollouts.parquet --output_dir ./filtered/

Usage:
    # Analyze rollouts with default settings (percentile bucketing)
    python scripts/analyze_rollouts.py \\
        --rollouts rollouts.parquet \\
        --model_path /path/to/model \\
        --output_dir ./difficulty_results

    # Use pass_rate bucketing strategy
    python scripts/analyze_rollouts.py \\
        --rollouts rollouts.parquet \\
        --model_path /path/to/model \\
        --output_dir ./difficulty_results \\
        --bucketing_strategy pass_rate

    # Use adaptive (K-means clustering) bucketing
    python scripts/analyze_rollouts.py \\
        --rollouts rollouts.parquet \\
        --model_path /path/to/model \\
        --output_dir ./difficulty_results \\
        --bucketing_strategy adaptive
"""

import argparse
import json
import os
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from transformers import AutoTokenizer

from verl import DataProto
from verl.utils.reward_score import default_compute_score
from verl.workers.reward_manager.naive import NaiveRewardManager


class RolloutAnalyzer:
    """Analyze existing rollouts - calculate rewards and perform difficulty filtering."""

    def __init__(
        self,
        rollouts_path: str,
        model_path: str,
        output_dir: str,
        bucketing_strategy: str = "percentile",
        reward_fn_key: str = "reward_model",
    ):
        """Initialize the rollout analyzer.

        Args:
            rollouts_path: Path to rollouts parquet/jsonl file
            model_path: Path to model (needed for tokenizer)
            output_dir: Directory to save results
            bucketing_strategy: Strategy for difficulty bucketing
            reward_fn_key: Key in data containing reward model info
        """
        self.rollouts_path = rollouts_path
        self.model_path = model_path
        self.output_dir = output_dir
        self.bucketing_strategy = bucketing_strategy
        self.reward_fn_key = reward_fn_key

        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)

        # Load tokenizer
        print(f"Loading tokenizer from {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Initialize reward manager
        print("Initializing reward manager")
        self.reward_fn = NaiveRewardManager(
            tokenizer=self.tokenizer,
            num_examine=1,
            compute_score=default_compute_score,
            reward_fn_key=self.reward_fn_key,
        )

    def load_rollouts(self) -> pd.DataFrame:
        """Load rollouts from parquet or jsonl file."""
        print(f"Loading rollouts from {self.rollouts_path}")

        if self.rollouts_path.endswith(".parquet"):
            df = pd.read_parquet(self.rollouts_path)
        elif self.rollouts_path.endswith(".jsonl"):
            df = pd.read_json(self.rollouts_path, lines=True)
        else:
            raise ValueError(f"Unsupported file format: {self.rollouts_path}")

        print(f"Loaded {len(df)} rollouts")
        return df

    def calculate_rewards(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate rewards for all rollouts.

        Args:
            df: DataFrame with columns: prompt, response, data_idx, sample_idx, original_*

        Returns:
            DataFrame with additional 'reward' column
        """
        print("Calculating rewards for all rollouts...")

        rewards = []
        for idx, row in tqdm(df.iterrows(), total=len(df)):
            # Tokenize prompt and response
            prompt_ids = self.tokenizer.encode(row["prompt"], add_special_tokens=True)
            response_ids = self.tokenizer.encode(row["response"], add_special_tokens=False)

            # Combine into single sequence
            input_ids = prompt_ids + response_ids
            prompt_length = len(prompt_ids)

            # Create attention mask and response mask
            attention_mask = [1] * len(input_ids)
            response_mask = [0] * prompt_length + [1] * len(response_ids)

            # Convert to tensors
            input_ids_tensor = torch.tensor([input_ids], dtype=torch.long)
            attention_mask_tensor = torch.tensor([attention_mask], dtype=torch.long)
            response_mask_tensor = torch.tensor([response_mask], dtype=torch.long)
            prompt_ids_tensor = torch.tensor([prompt_ids], dtype=torch.long)

            # Create DataProto batch
            batch = {
                "input_ids": input_ids_tensor,
                "attention_mask": attention_mask_tensor,
                "prompts": prompt_ids_tensor,  # NaiveRewardManager expects this
                "responses": input_ids_tensor,  # Full sequence for reward calculation
                "response_mask": response_mask_tensor,
            }

            # Extract reward model data from original columns
            reward_model_data = {}
            for col in df.columns:
                if col.startswith("original_"):
                    key = col.replace("original_", "")
                    reward_model_data[key] = row[col]

            # Add reward model data to non_tensor_batch
            non_tensor_batch = {self.reward_fn_key: [reward_model_data]}

            # Create DataProto
            data_proto = DataProto(batch=batch, non_tensor_batch=non_tensor_batch, meta_info={"rl_step": 0})

            # Calculate reward
            try:
                reward_result = self.reward_fn(data_proto, return_dict=True)
                reward = reward_result["reward_tensor"].item()
            except Exception as e:
                print(f"Error calculating reward for idx {idx}: {e}")
                reward = 0.0

            rewards.append(reward)

        # Add rewards to dataframe
        df["reward"] = rewards
        print(f"Calculated {len(rewards)} rewards")

        return df

    def compute_mean_at_k(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """Compute mean@k metrics per problem.

        Args:
            df: DataFrame with columns: data_idx, reward

        Returns:
            Dictionary mapping data_idx to mean@k metrics
        """
        print("Computing mean@k metrics per problem...")

        # Group by data_idx
        grouped = df.groupby("data_idx")

        metrics = {}
        for data_idx, group in grouped:
            rewards = sorted(group["reward"].tolist(), reverse=True)
            num_samples = len(rewards)

            # Calculate mean@k for different k values
            mean_at_k = {}
            for k in [1, 3, 5, 10, 20]:
                if k <= num_samples:
                    mean_at_k[f"mean@{k}"] = np.mean(rewards[:k])
                else:
                    mean_at_k[f"mean@{k}"] = np.mean(rewards)  # Use all available

            # Also compute pass rate (assuming reward >= 0.5 means pass)
            pass_rate = np.mean([1 if r >= 0.5 else 0 for r in rewards])

            metrics[data_idx] = {
                **mean_at_k,
                "pass_rate": pass_rate,
                "mean_reward": np.mean(rewards),
                "max_reward": np.max(rewards),
                "min_reward": np.min(rewards),
                "std_reward": np.std(rewards),
                "num_samples": num_samples,
            }

        print(f"Computed metrics for {len(metrics)} problems")
        return metrics

    def bucket_by_percentile(self, metrics: Dict[str, Dict[str, float]]) -> Dict[str, str]:
        """Bucket problems by percentile of mean@5.

        Args:
            metrics: Dictionary mapping data_idx to metrics

        Returns:
            Dictionary mapping data_idx to bucket name
        """
        # Extract mean@5 values
        mean_at_5_values = [m["mean@5"] for m in metrics.values()]
        data_indices = list(metrics.keys())

        # Calculate percentiles
        percentiles = np.percentile(mean_at_5_values, [25, 50, 75])

        # Assign buckets
        buckets = {}
        for data_idx, mean_at_5 in zip(data_indices, mean_at_5_values):
            if mean_at_5 <= percentiles[0]:
                buckets[data_idx] = "hard"
            elif mean_at_5 <= percentiles[1]:
                buckets[data_idx] = "medium-hard"
            elif mean_at_5 <= percentiles[2]:
                buckets[data_idx] = "medium-easy"
            else:
                buckets[data_idx] = "easy"

        return buckets

    def bucket_by_pass_rate(self, metrics: Dict[str, Dict[str, float]]) -> Dict[str, str]:
        """Bucket problems by pass rate.

        Args:
            metrics: Dictionary mapping data_idx to metrics

        Returns:
            Dictionary mapping data_idx to bucket name
        """
        buckets = {}
        for data_idx, metric in metrics.items():
            pass_rate = metric["pass_rate"]

            if pass_rate == 0:
                buckets[data_idx] = "very_hard"
            elif pass_rate < 0.25:
                buckets[data_idx] = "hard"
            elif pass_rate < 0.5:
                buckets[data_idx] = "medium"
            elif pass_rate < 0.75:
                buckets[data_idx] = "easy"
            else:
                buckets[data_idx] = "very_easy"

        return buckets

    def bucket_by_mean_reward(self, metrics: Dict[str, Dict[str, float]]) -> Dict[str, str]:
        """Bucket problems by mean reward across all samples.

        Args:
            metrics: Dictionary mapping data_idx to metrics

        Returns:
            Dictionary mapping data_idx to bucket name
        """
        # Extract mean rewards
        mean_rewards = [m["mean_reward"] for m in metrics.values()]
        data_indices = list(metrics.keys())

        # Calculate quartiles
        quartiles = np.percentile(mean_rewards, [25, 50, 75])

        # Assign buckets
        buckets = {}
        for data_idx, mean_reward in zip(data_indices, mean_rewards):
            if mean_reward <= quartiles[0]:
                buckets[data_idx] = "very_hard"
            elif mean_reward <= quartiles[1]:
                buckets[data_idx] = "hard"
            elif mean_reward <= quartiles[2]:
                buckets[data_idx] = "medium"
            else:
                buckets[data_idx] = "easy"

        return buckets

    def bucket_by_adaptive(self, metrics: Dict[str, Dict[str, float]]) -> Dict[str, str]:
        """Bucket problems using K-means clustering on multiple features.

        Args:
            metrics: Dictionary mapping data_idx to metrics

        Returns:
            Dictionary mapping data_idx to bucket name
        """
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler

        # Extract features for clustering
        data_indices = list(metrics.keys())
        features = []
        for data_idx in data_indices:
            m = metrics[data_idx]
            features.append([m["mean@5"], m["pass_rate"], m["std_reward"]])

        features = np.array(features)

        # Standardize features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # K-means clustering with 4 clusters
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(features_scaled)

        # Sort clusters by mean@5 (lower mean@5 = harder)
        cluster_means = []
        for i in range(4):
            mask = cluster_labels == i
            cluster_mean = np.mean([metrics[data_indices[j]]["mean@5"] for j in range(len(data_indices)) if mask[j]])
            cluster_means.append((i, cluster_mean))

        cluster_means.sort(key=lambda x: x[1])  # Sort by mean@5

        # Map clusters to difficulty names
        cluster_to_difficulty = {
            cluster_means[0][0]: "very_hard",
            cluster_means[1][0]: "hard",
            cluster_means[2][0]: "medium",
            cluster_means[3][0]: "easy",
        }

        # Assign buckets
        buckets = {}
        for data_idx, cluster_label in zip(data_indices, cluster_labels):
            buckets[data_idx] = cluster_to_difficulty[cluster_label]

        return buckets

    def perform_bucketing(self, metrics: Dict[str, Dict[str, float]]) -> Dict[str, str]:
        """Perform difficulty bucketing based on strategy.

        Args:
            metrics: Dictionary mapping data_idx to metrics

        Returns:
            Dictionary mapping data_idx to bucket name
        """
        print(f"Performing bucketing using strategy: {self.bucketing_strategy}")

        if self.bucketing_strategy == "percentile":
            buckets = self.bucket_by_percentile(metrics)
        elif self.bucketing_strategy == "pass_rate":
            buckets = self.bucket_by_pass_rate(metrics)
        elif self.bucketing_strategy == "mean_reward":
            buckets = self.bucket_by_mean_reward(metrics)
        elif self.bucketing_strategy == "adaptive":
            buckets = self.bucket_by_adaptive(metrics)
        else:
            raise ValueError(f"Unknown bucketing strategy: {self.bucketing_strategy}")

        return buckets

    def save_results(
        self, df: pd.DataFrame, metrics: Dict[str, Dict[str, float]], buckets: Dict[str, str]
    ) -> None:
        """Save analysis results to output directory.

        Args:
            df: DataFrame with rewards
            metrics: Dictionary mapping data_idx to metrics
            buckets: Dictionary mapping data_idx to bucket name
        """
        print(f"Saving results to {self.output_dir}")

        # Save rollouts with rewards
        rollouts_output = os.path.join(self.output_dir, "rollouts_with_rewards.parquet")
        df.to_parquet(rollouts_output, index=False)
        print(f"Saved rollouts with rewards to {rollouts_output}")

        # Save metrics per problem
        metrics_output = os.path.join(self.output_dir, "problem_metrics.json")
        with open(metrics_output, "w") as f:
            json.dump(metrics, f, indent=2, default=int)  # default=int for numpy int types
        print(f"Saved problem metrics to {metrics_output}")

        # Save buckets
        buckets_output = os.path.join(self.output_dir, "problem_buckets.json")
        with open(buckets_output, "w") as f:
            json.dump(buckets, f, indent=2, default=int)
        print(f"Saved problem buckets to {buckets_output}")

        # Create bucketed datasets
        for bucket_name in set(buckets.values()):
            bucket_data_indices = [data_idx for data_idx, b in buckets.items() if b == bucket_name]
            bucket_df = df[df["data_idx"].isin(bucket_data_indices)]

            bucket_output = os.path.join(self.output_dir, f"rollouts_{bucket_name}.parquet")
            bucket_df.to_parquet(bucket_output, index=False)
            print(f"Saved {len(bucket_df)} rollouts to {bucket_output}")

        # Create summary statistics
        summary = {
            "total_problems": len(metrics),
            "total_rollouts": len(df),
            "bucketing_strategy": self.bucketing_strategy,
            "bucket_distribution": {bucket_name: len([b for b in buckets.values() if b == bucket_name]) for bucket_name in set(buckets.values())},
            "overall_metrics": {
                "mean_pass_rate": np.mean([m["pass_rate"] for m in metrics.values()]),
                "mean_reward": np.mean([m["mean_reward"] for m in metrics.values()]),
                "mean_at_5": np.mean([m["mean@5"] for m in metrics.values()]),
            },
        }

        summary_output = os.path.join(self.output_dir, "summary.json")
        with open(summary_output, "w") as f:
            json.dump(summary, f, indent=2)
        print(f"Saved summary to {summary_output}")

        # Print summary
        print("\n" + "=" * 60)
        print("ANALYSIS SUMMARY")
        print("=" * 60)
        print(f"Total problems: {summary['total_problems']}")
        print(f"Total rollouts: {summary['total_rollouts']}")
        print(f"Bucketing strategy: {summary['bucketing_strategy']}")
        print("\nBucket distribution:")
        for bucket_name, count in summary["bucket_distribution"].items():
            print(f"  {bucket_name}: {count} problems")
        print("\nOverall metrics:")
        print(f"  Mean pass rate: {summary['overall_metrics']['mean_pass_rate']:.3f}")
        print(f"  Mean reward: {summary['overall_metrics']['mean_reward']:.3f}")
        print(f"  Mean@5: {summary['overall_metrics']['mean_at_5']:.3f}")
        print("=" * 60)

    def run(self) -> None:
        """Run the full analysis pipeline."""
        # Load rollouts
        df = self.load_rollouts()

        # Calculate rewards
        df = self.calculate_rewards(df)

        # Compute mean@k metrics
        metrics = self.compute_mean_at_k(df)

        # Perform difficulty bucketing
        buckets = self.perform_bucketing(metrics)

        # Save results
        self.save_results(df, metrics, buckets)

        print("\nAnalysis complete!")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Analyze existing rollouts - calculate rewards and filter by difficulty")

    # Required arguments
    parser.add_argument("--rollouts", type=str, required=True, help="Path to rollouts parquet/jsonl file")
    parser.add_argument("--model_path", type=str, required=True, help="Path to model (for tokenizer)")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to save results")

    # Optional arguments
    parser.add_argument(
        "--bucketing_strategy",
        type=str,
        default="percentile",
        choices=["percentile", "pass_rate", "mean_reward", "adaptive"],
        help="Strategy for difficulty bucketing",
    )
    parser.add_argument(
        "--reward_fn_key", type=str, default="reward_model", help="Key in data containing reward model info"
    )

    args = parser.parse_args()

    # Create analyzer
    analyzer = RolloutAnalyzer(
        rollouts_path=args.rollouts,
        model_path=args.model_path,
        output_dir=args.output_dir,
        bucketing_strategy=args.bucketing_strategy,
        reward_fn_key=args.reward_fn_key,
    )

    # Run analysis
    analyzer.run()


if __name__ == "__main__":
    main()
