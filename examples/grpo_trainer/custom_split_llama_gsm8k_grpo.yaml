hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

# Data configuration
data:
  train_files: ${oc.env:HOME}/data/gsm8k/train.parquet
  val_files: ${oc.env:HOME}/data/gsm8k/test.parquet
  train_batch_size: 256
  val_batch_size: 1024
  max_prompt_length: 512
  max_response_length: 1024
  filter_overlong_prompts: True
  truncation: 'error'
  shuffle: False

# Actor, Rollout, and Reference model configuration
actor_rollout_ref:
  # Model configuration for Custom Split LLaMA
  model:
    # Path to your Custom Split LLaMA model directory
    # This should contain:
    # - config.json (with Custom Split LLaMA config)
    # - 8B model checkpoint
    # - 70B model checkpoint
    path: ${oc.env:MODEL_PATH}  # Set via environment variable

    # Custom Split LLaMA specific parameters
    # These will be loaded from config.json in the model path
    # Ensure your config.json has:
    # - architectures: ["CustomSplitLLamaForCausalLM"]
    # - path8b: "/path/to/llama-8b-model"
    # - path70b: "/path/to/llama-70b-model"
    # - num_layers_8: 32
    # - num_layers_70: 8
    # - mlp: false
    # - vocab_size: 128256

    # Training optimizations
    use_remove_padding: True
    enable_gradient_checkpointing: True

    # Optional: LoRA fine-tuning (uncomment to enable)
    # lora_rank: 64
    # lora_alpha: 32
    # lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

  # Actor (policy model) configuration
  actor:
    # Optimizer settings
    optim:
      lr: 1e-6  # Learning rate
      weight_decay: 0.01
      eps: 1e-8

    # PPO/GRPO settings
    ppo_mini_batch_size: 256
    ppo_micro_batch_size_per_gpu: 16
    use_kl_loss: True
    kl_loss_coef: 0.001
    kl_loss_type: low_var_kl
    entropy_coeff: 0

    # FSDP configuration for actor
    fsdp_config:
      param_offload: False
      optimizer_offload: False
      grad_offload: False

  # Rollout (generation) configuration
  rollout:
    name: vllm  # Use vLLM for fast generation
    log_prob_micro_batch_size_per_gpu: 16

    # Parallelism for rollout
    tensor_model_parallel_size: 4  # Adjust based on your GPU memory
    pipeline_model_parallel_size: 1

    # vLLM specific settings
    gpu_memory_utilization: 0.85
    n: 5  # Number of responses per prompt for GRPO
    load_format: safetensors
    layered_summon: True

    # Temperature and sampling
    temperature: 1.0
    top_p: 1.0
    top_k: -1

  # Reference model configuration (for KL divergence)
  ref:
    log_prob_micro_batch_size_per_gpu: 16

    # FSDP configuration for reference model
    fsdp_config:
      param_offload: True  # Offload to save memory
      optimizer_offload: False
      grad_offload: False

# GRPO algorithm configuration
algorithm:
  adv_estimator: grpo
  use_kl_in_reward: False

  # GRPO specific parameters
  grpo:
    group_size: 5  # Should match rollout.n
    temperature: 1.0

# Trainer configuration
trainer:
  # Training parameters
  total_epochs: 20
  n_gpus_per_node: 8
  nnodes: 1

  # Validation and checkpointing
  val_before_train: False
  test_freq: 5
  save_freq: 10
  critic_warmup: 0

  # Logging
  logger: '["console","wandb"]'
  project_name: 'custom_split_llama_grpo_gsm8k'
  experiment_name: 'custom_split_llama_8b_70b_grpo'

  # Default save directory
  default_hdfs_dir: null
  default_local_dir: ./checkpoints

# Resource management
ray:
  placement_group:
    bundles:
      - GPU: 1
        CPU: 2
