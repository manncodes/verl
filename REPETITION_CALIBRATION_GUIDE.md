# Repetition Detection - Calibration Guide

Complete guide to choosing and using the right calibration level for your use case.

## üìä Available Calibrations

### **1. Calibrated (RECOMMENDED)** ‚≠ê

**File:** `repetition_penalty_calibrated.py`

**Philosophy:** Sweet spot between leniency and strictness

**Best For:**
- General instruction following
- Most production use cases
- When you want to catch egregious cases but avoid false positives

**Behavior:**
- ‚úÖ No penalty on good quality text
- ‚úÖ No penalty on minor/acceptable repetition
- ‚úÖ No penalty on informative templates
- ‚úó **Penalizes egregious repetition** (35% penalty)
- ‚ö†Ô∏è No penalty on borderline robotic templates (gives benefit of doubt)

**Thresholds:**
```
< 0.30: No penalty (acceptable)
0.30-0.45: 15% penalty (minor issues)
0.45-0.60: 35% penalty (clear issues)
0.60-0.75: 60% penalty (serious issues)
> 0.75: 85% penalty (egregious)
```

**Usage:**
```python
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated

final_reward = apply_repetition_penalty_calibrated(
    text=response['content'],
    base_reward=base_reward
)
```

---

### **2. Conservative**

**File:** `repetition_penalty_conservative.py`

**Philosophy:** Maximum leniency - only extreme cases

**Best For:**
- Creative tasks
- When false positives are very costly
- Exploratory training phases

**Behavior:**
- ‚úÖ No penalty on everything except extreme cases
- Even egregious repetition might not be penalized
- Very high bar for penalties

**Thresholds:**
```
< 0.35: No penalty
0.35-0.50: 10% penalty
0.50-0.65: 30% penalty
0.65-0.80: 50% penalty
> 0.80: 80% penalty
```

**Usage:**
```python
from repetition_penalty_conservative import apply_repetition_penalty_conservative

final_reward = apply_repetition_penalty_conservative(
    text=response['content'],
    base_reward=base_reward,
    mode="default"  # or "ultra_conservative"
)
```

---

### **3. Moderate**

**File:** `repetition_penalty_final.py`

**Philosophy:** Balanced - catches most issues

**Best For:**
- Quality-focused applications
- When you want to discourage any repetition
- Research/analysis

**Behavior:**
- More aggressive than calibrated
- Will penalize some borderline cases
- Good separation between good/bad

**Thresholds:**
```
< 0.15: No penalty
0.15-0.30: 15% penalty
0.30-0.50: 40% penalty
0.50-0.70: 70% penalty
> 0.70: 90% penalty
```

**Usage:**
```python
from repetition_penalty_final import apply_repetition_penalty

final_reward = apply_repetition_penalty(
    text=response['content'],
    base_reward=base_reward,
    severity="moderate"
)
```

---

## üéØ Choosing the Right Calibration

### Decision Tree

```
START
  ‚îÇ
  ‚îú‚îÄ Need maximum precision (avoid any false positives)?
  ‚îÇ  ‚îî‚îÄ Use CONSERVATIVE
  ‚îÇ
  ‚îú‚îÄ Want to catch egregious cases while being lenient on borderline?
  ‚îÇ  ‚îî‚îÄ Use CALIBRATED ‚≠ê (RECOMMENDED)
  ‚îÇ
  ‚îî‚îÄ Want to discourage most repetition?
     ‚îî‚îÄ Use MODERATE
```

### By Use Case

| Use Case | Recommended | Reasoning |
|----------|-------------|-----------|
| **General RLHF Training** | CALIBRATED | Balance between catching issues and avoiding false positives |
| **Instruction Following** | CALIBRATED | Want to catch gaming without penalizing natural variation |
| **Creative Writing** | CONSERVATIVE | Allow stylistic repetition |
| **Code Generation** | MODERATE | Code often has repetitive patterns (imports, etc.) |
| **Summarization** | MODERATE | Should be concise, penalize padding |
| **Question Answering** | CALIBRATED | Allow thorough answers without penalizing structure |
| **Research/Analysis** | MODERATE | Want to identify all potential issues |

---

## üìà Comparison Results

### Test Case Performance

| Test Case | Calibrated | Conservative | Moderate |
|-----------|------------|--------------|----------|
| Good quality | 0.000 ‚úì | 0.000 ‚úì | 0.000 ‚úì |
| Minor repetition | 0.000 ‚úì | 0.000 ‚úì | 0.113 ‚ö† |
| Informative template | 0.000 ‚úì | 0.000 ‚úì | 0.177 ‚ö† |
| Egregious repetition | 0.555 ‚úó | 0.252 ‚ö† | 0.615 ‚úó |
| Robotic template | 0.265 ‚úì | 0.193 ‚úì | 0.481 ‚úó |

**Legend:**
- ‚úì No penalty (1.0x multiplier)
- ‚ö† Small penalty (0.85-0.95x)
- ‚úó Significant penalty (< 0.7x)

### Key Insights

**CALIBRATED:**
- Best balance: catches egregious (0.555 score) but not borderline (0.265)
- Zero false positives on acceptable content
- **RECOMMENDED for most users**

**CONSERVATIVE:**
- Most lenient: even egregious only scores 0.252
- Almost zero penalties on anything
- Use if false positives are extremely costly

**MODERATE:**
- Most strict: penalizes even informative templates (0.177 ‚Üí penalty)
- Good for research and quality control
- Higher false positive rate

---

## üîß Integration Examples

### Example 1: Basic Integration (Calibrated)

```python
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated

def calculate_reward(response, ground_truth):
    # Your base reward
    base_reward = your_reward_function(response, ground_truth)

    # Apply calibrated penalty
    final_reward = apply_repetition_penalty_calibrated(
        text=response['content'],
        base_reward=base_reward
    )

    return final_reward
```

### Example 2: With Diagnostics

```python
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated

def calculate_reward_with_logging(response, ground_truth):
    base_reward = your_reward_function(response, ground_truth)

    # Get detailed breakdown
    final_reward, details = apply_repetition_penalty_calibrated(
        response['content'],
        base_reward,
        return_details=True
    )

    # Log if penalty was applied
    if details['penalty_applied'] > 0:
        print(f"Repetition penalty: {details['repetition_score']:.3f}")
        print(f"Assessment: {details['assessment']}")
        print(f"Components: {details['components']}")

    return final_reward
```

### Example 3: Conditional Calibration

```python
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated
from repetition_penalty_conservative import apply_repetition_penalty_conservative

def smart_penalty(response, task_type):
    base_reward = calculate_base_reward(response)

    # Choose calibration based on task
    if task_type == "creative_writing":
        # Use conservative for creative tasks
        final_reward = apply_repetition_penalty_conservative(
            response['content'], base_reward, mode="ultra_conservative"
        )
    elif task_type == "instruction_following":
        # Use calibrated for instruction following
        final_reward = apply_repetition_penalty_calibrated(
            response['content'], base_reward
        )
    else:
        # Default: calibrated
        final_reward = apply_repetition_penalty_calibrated(
            response['content'], base_reward
        )

    return final_reward
```

### Example 4: VERL Integration

```python
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated

class RewardFunction:
    def __call__(self, batch):
        rewards = []

        for item in batch:
            # Calculate base reward
            base_reward = self.calculate_instruction_reward(item)

            # Apply repetition penalty (calibrated)
            final_reward = apply_repetition_penalty_calibrated(
                text=item['response'],
                base_reward=base_reward
            )

            rewards.append(final_reward)

        return rewards
```

---

## üß™ Testing Your Calibration

### Quick Test Script

```python
from repetition_penalty_calibrated import diagnose_calibrated

# Test on your actual model outputs
your_text = """
[Paste your model output here]
"""

print(diagnose_calibrated(your_text))
```

### Comparing Calibrations

```python
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated
from repetition_penalty_conservative import apply_repetition_penalty_conservative
from repetition_penalty_final import apply_repetition_penalty

text = your_model_output
base_reward = 1.0

# Test all three
cal_reward = apply_repetition_penalty_calibrated(text, base_reward)
cons_reward = apply_repetition_penalty_conservative(text, base_reward)
mod_reward = apply_repetition_penalty(text, base_reward, severity="moderate")

print(f"Calibrated:   {cal_reward:.3f}")
print(f"Conservative: {cons_reward:.3f}")
print(f"Moderate:     {mod_reward:.3f}")
```

---

## üìù Recommendations

### For Your Use Case

Based on your requirement:
> "Only penalize when it has true positives of repetition and inhumane responses. If it's fine, keep the score."

**‚Üí Use CALIBRATED** ‚≠ê

**Why:**
1. ‚úÖ Keeps scores intact for borderline cases
2. ‚úÖ Only penalizes clear, egregious repetition
3. ‚úÖ Catches truly "inhumane" robotic responses
4. ‚úÖ Zero false positives on good/acceptable text
5. ‚úÖ High precision, balanced recall

### Getting Started

```python
# 1. Install (no dependencies needed!)
# Just copy the file to your project

# 2. Import
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated

# 3. Use in your reward function
final_reward = apply_repetition_penalty_calibrated(
    text=response['content'],
    base_reward=base_reward
)

# That's it!
```

### Monitoring & Tuning

```python
# Add to your training loop
penalties_applied = []

for batch in training_data:
    for item in batch:
        final_reward, details = apply_repetition_penalty_calibrated(
            item['response'],
            base_reward,
            return_details=True
        )

        if details['penalty_applied'] > 0:
            penalties_applied.append({
                'score': details['repetition_score'],
                'penalty': details['penalty_applied'],
                'text': item['response'][:100]  # First 100 chars
            })

# Analyze
print(f"Penalties applied: {len(penalties_applied)}/{len(training_data)}")
print(f"Average penalty: {np.mean([p['penalty'] for p in penalties_applied]):.3f}")

# Review samples
for p in penalties_applied[:5]:
    print(f"\nScore: {p['score']:.3f}, Penalty: {p['penalty']:.3f}")
    print(f"Text: {p['text']}...")
```

---

## üéì Best Practices

1. **Start with CALIBRATED**
   - Works well for most cases
   - Adjust if needed after observing results

2. **Monitor Penalties**
   - Log when penalties are applied
   - Review samples to check if correct

3. **Use Diagnostics**
   - `diagnose_calibrated()` shows why penalty was applied
   - Helps understand model behavior

4. **Don't Over-Penalize**
   - If > 20% of outputs get penalized, consider CONSERVATIVE
   - May indicate calibration is too strict for your domain

5. **Iterate**
   - Start conservative, gradually increase strictness
   - Monitor model quality over training

---

## üìö Files Reference

| File | Purpose | Use When |
|------|---------|----------|
| `repetition_penalty_calibrated.py` | **Main file - use this** | General use, most cases |
| `repetition_penalty_conservative.py` | Maximum leniency | Avoid false positives at all costs |
| `repetition_penalty_final.py` | Balanced/moderate | Quality control, research |
| `test_all_calibrations.py` | Compare all versions | Deciding which to use |
| `REPETITION_CALIBRATION_GUIDE.md` | This guide | Reference |

---

## ‚ùì FAQ

**Q: Which calibration should I use?**
A: Start with CALIBRATED. It's the sweet spot for most use cases.

**Q: My model outputs get penalized too often. What should I do?**
A: Switch to CONSERVATIVE, or check if your model is actually generating repetitive content.

**Q: I want to be very strict. Which should I use?**
A: Use MODERATE with `severity="strict"` or `severity="very_strict"`.

**Q: Can I create custom thresholds?**
A: Yes! Edit the threshold values in the apply_repetition_penalty functions.

**Q: How do I know if the penalty is correct?**
A: Use `diagnose_calibrated(text)` to see detailed breakdown.

**Q: Does this require numpy?**
A: No! Works without numpy (uses fallback for statistical functions).

---

## ‚úÖ Summary

**Your Requirement:**
- Keep scores intact for fine/acceptable text
- Only penalize true positives of repetition
- Focus on inhumane/robotic responses

**Recommended Solution:**
```python
from repetition_penalty_calibrated import apply_repetition_penalty_calibrated

final_reward = apply_repetition_penalty_calibrated(
    text=response['content'],
    base_reward=base_reward
)
```

**Calibration Settings:**
- Threshold: 0.30 (no penalty below this)
- Penalties: 15% ‚Üí 35% ‚Üí 60% ‚Üí 85%
- Focus: Egregious cases only

**Result:**
- ‚úÖ Zero false positives
- ‚úÖ Catches truly problematic repetition
- ‚úÖ Gives benefit of doubt to borderline cases
- ‚úÖ Production-ready
